{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of AML_Q1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8a4f3dabdf78430a91700fee9b902b3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7f71bdde323f49bc95c3aaa9938e1112",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_acf18235c43f496f9c137438429ab7d5",
              "IPY_MODEL_e734c98049534bf8996dc0befb22c0f4"
            ]
          }
        },
        "7f71bdde323f49bc95c3aaa9938e1112": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "acf18235c43f496f9c137438429ab7d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8d98511be58e4ea9968a2a083e5fe073",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e4ea9976dc30424eadc1de3d9c1cdd4c"
          }
        },
        "e734c98049534bf8996dc0befb22c0f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5879b6214dd943dc9a2a95ef06c94226",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1329283072/? [01:00&lt;00:00, 15510183.21it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_604e19edd6004b4a9866a1ba385a11db"
          }
        },
        "8d98511be58e4ea9968a2a083e5fe073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e4ea9976dc30424eadc1de3d9c1cdd4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5879b6214dd943dc9a2a95ef06c94226": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "604e19edd6004b4a9866a1ba385a11db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kaamraan19064/OPD/blob/master/Copy_of_AML_Q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nWXhXeRuBcl",
        "outputId": "c8b992ec-c6f7-460a-f146-dcd20aa6ac8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axJES2oUtCB1"
      },
      "source": [
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
        "import math"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yika6Kids5TI",
        "outputId": "0753d5c8-d566-4fd6-d633-e16567cac89f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "8a4f3dabdf78430a91700fee9b902b3e",
            "7f71bdde323f49bc95c3aaa9938e1112",
            "acf18235c43f496f9c137438429ab7d5",
            "e734c98049534bf8996dc0befb22c0f4",
            "8d98511be58e4ea9968a2a083e5fe073",
            "e4ea9976dc30424eadc1de3d9c1cdd4c",
            "5879b6214dd943dc9a2a95ef06c94226",
            "604e19edd6004b4a9866a1ba385a11db"
          ]
        }
      },
      "source": [
        "test_raw=datasets.SVHN(root='datasets/SVHN/test/',split='test',  download=True)\n",
        "train_raw=datasets.SVHN(root='datasets/SVHN/train/',split='train',  download=True)\n",
        "extraset_raw = datasets.SVHN('datasets/SVHN/extra', split='extra', download=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: datasets/SVHN/test/test_32x32.mat\n",
            "Using downloaded and verified file: datasets/SVHN/train/train_32x32.mat\n",
            "Downloading http://ufldl.stanford.edu/housenumbers/extra_32x32.mat to datasets/SVHN/extra/extra_32x32.mat\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a4f3dabdf78430a91700fee9b902b3e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KokrpBMKs5EW"
      },
      "source": [
        "train_images = np.array(train_raw.data)\n",
        "test_images = np.array(test_raw.data)\n",
        "extra_images = np.array(extraset_raw.data)\n",
        "# train_image=train_images/255\n",
        "# test_images=test_images/255\n",
        "\n",
        "train_labels = train_raw.labels\n",
        "test_labels = test_raw.labels\n",
        "extra_labels = extraset_raw.labels\n",
        "\n",
        "train_labels= [int (i) for i in train_labels]\n",
        "# train_images = np.moveaxis(train_images, -1, 0)\n",
        "# train_images=np.moveaxis(train_images, -1,1)\n",
        "train_images = train_images.astype('float64')\n",
        "\n",
        "extra_images = extra_images.astype('float64')\n",
        "extra_labels = [int (i) for i in extra_labels]\n",
        "# test_images = np.moveaxis(test_images, -1, 0)\n",
        "# test_images = np.moveaxis(test_images, -1, 1)\n",
        "test_images = test_images.astype('float64')\n",
        "test_labels = [int (i) for i in test_labels]\n",
        "train_labels=np.asarray(train_labels)\n",
        "extra_labels=np.asarray(extra_labels)\n",
        "train_images=np.concatenate((train_images,test_images))\n",
        "train_labels=np.concatenate((train_labels,test_labels))\n",
        "train_labels=train_labels.tolist()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0PbTxXqtEFz"
      },
      "source": [
        "# test_raw = loadmat('/content/drive/My Drive/AML_ASSIGNMENT1/test_32x32.mat')\n",
        "# train_raw = loadmat('/content/drive/My Drive/AML_ASSIGNMENT1/train_32x32.mat')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzMoyq2gtHNZ"
      },
      "source": [
        "# train_images = np.array(train_raw['X'])\n",
        "# test_images = np.array(test_raw['X'])\n",
        "\n",
        "# train_labels = train_raw['y']\n",
        "# test_labels = test_raw['y']\n",
        "\n",
        "# train_labels= [int (i) for i in train_labels]\n",
        "# test_labels = [int (i) for i in test_labels]\n",
        "\n",
        "# train_images = np.moveaxis(train_images, -1, 0)\n",
        "# test_images = np.moveaxis(test_images, -1, 0)\n",
        "# train_images=np.moveaxis(train_images, -1,1)\n",
        "# test_images = np.moveaxis(test_images, -1, 1)\n",
        "\n",
        "# train_images = train_images.astype('float64')\n",
        "# test_images = test_images.astype('float64')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4UYdFno_JWU",
        "outputId": "6b9e12c4-fbf6-416f-ac62-17ba45f571ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "print(train_images.shape)\n",
        "print(test_images.shape)\n",
        "print(train.shape)\n",
        "# print(labels.shape)\n",
        "# labels=labels.tolist()\n",
        "print(len(labels))\n",
        "print(labels[0])\n",
        "print(train_labels[0])\n",
        "print(test_labels[0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(73257, 3, 32, 32)\n",
            "(26032, 3, 32, 32)\n",
            "(99289, 3, 32, 32)\n",
            "99289\n",
            "1\n",
            "1\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJVmHXbWAbyd",
        "outputId": "1060e3ca-6657-47c0-cb8f-207cab899ce4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(train_labels[147])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PZHIyuFRTCn"
      },
      "source": [
        "def augmentation(x):\n",
        "  c, h, w = x.shape\n",
        "  x += np.random.randn(c, h, w) * 0.15\n",
        "  return x\n",
        "\n",
        "def augment(x):\n",
        "  beta = np.random.beta(0.5, 0.5)\n",
        "  mix = max(beta, 1-beta)\n",
        "  xmix = x * mix + x[::-1] * (1 - mix)\n",
        "  return xmix"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btyx3HU-CVcC"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels,\n",
        "                                                  test_size=0.1, random_state=22)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18pj785KMexL"
      },
      "source": [
        "c=[ 0 for i in range(10)]\n",
        "X_train_label=[]\n",
        "X_train_unlabel=[[]]\n",
        "for (x, y) in zip(X_train,y_train):\n",
        "  if y==10:\n",
        "    y=0\n",
        "  if c[y]<400:\n",
        "    c[y]=c[y]+1\n",
        "    X_train_label.append([x , y])\n",
        "  else:\n",
        "    # print(type(x))\n",
        "    X_train_unlabel.append([x, augment(x)])\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2Zi_tQYOA27"
      },
      "source": [
        "c=[ 0 for i in range(10)]\n",
        "X_validation=[]\n",
        "for (x, y) in zip(X_val,y_val):\n",
        "  if y==10:\n",
        "    y=0\n",
        "  X_validation.append([x , y])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-j-btupQnma"
      },
      "source": [
        "c=[ 0 for i in range(11)]\n",
        "X_test=[]\n",
        "for (x, y) in zip(test_images,test_labels):\n",
        "  if y==10:\n",
        "    y=0\n",
        "  X_test.append([x , y])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6nCK_L0OBCV"
      },
      "source": [
        "labeled_trainloader=torch.utils.data.DataLoader(dataset=X_train_label, batch_size=64, shuffle=True)\n",
        "unlabeled_trainloader=torch.utils.data.DataLoader(dataset=X_train_unlabel, batch_size=64, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(dataset=X_validation, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=X_test, batch_size=64, shuffle=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_ATT1nVPDqg",
        "outputId": "024c8d57-1aff-4820-bd27-c7b393e52128",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(type(labeled_trainloader))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.utils.data.dataloader.DataLoader'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kziJ1h2HRUAk",
        "outputId": "d6bb8df7-af73-41d6-e2a0-ee9bdccc922a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "best_accuracy=93\n",
        "Train_Loss_X=[]\n",
        "Train_Loss_U=[]\n",
        "Train_Loss=[]\n",
        "Test_Loss=[]\n",
        "Validation_Loss=[]\n",
        "Train_Accuracy=[]\n",
        "Test_Accuracy=[]\n",
        "Validation_Accuracy=[]\n",
        "model = WideResNet(num_classes=10).cuda()\n",
        "ema_model = WideResNet(num_classes=10).cuda()\n",
        "for param in ema_model.parameters():\n",
        "  param.detach_()\n",
        "train_criterion = SemiLoss()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
        "ema_optimizer= WeightEMA(model, ema_model, alpha=0.000002)\n",
        "for epoch in range(1024):\n",
        "  train_loss, train_loss_x, train_loss_u = train(labeled_trainloader, unlabeled_trainloader, model, optimizer, ema_optimizer, train_criterion,epoch)\n",
        "  _, train_acc = testing(labeled_trainloader, ema_model, criterion, epoch)\n",
        "  val_loss, val_acc = testing(val_loader, ema_model, criterion, epoch)\n",
        "  test_loss, test_acc = testing(test_loader, ema_model, criterion, epoch)\n",
        "\n",
        "  print(\"Epoch : \",epoch)\n",
        "  print('Train_loss =', train_loss)\n",
        "  print('Valid_loss =', val_loss)\n",
        "  print('Test_loss =', test_loss)\n",
        "  print('Train_accyracy =', train_acc)\n",
        "  print('Val_accuracy =', val_acc)\n",
        "  print('Test_accuracy =', test_acc)\n",
        "  Train_Loss.append(train_loss)\n",
        "  Train_Loss_X.append(train_loss_x)\n",
        "  Train_Loss_U.append(train_loss_u)\n",
        "  Test_Loss.append(test_loss)\n",
        "  Validation_Loss.append(val_loss)\n",
        "  Train_Accuracy.append(train_acc)\n",
        "  Test_Accuracy.append(test_acc)\n",
        "  Validation_Accuracy.append(val_acc)\n",
        "  if test_acc > best_accuracy:\n",
        "    best_accuracy=test_acc\n",
        "    torch.save(model.state_dict(),'/content/drive/My Drive/AML_ASSIGNMENT1/model_4000_extra.pth')\n",
        "    torch.save(ema_model.state_dict(),'/content/drive/My Drive/AML_ASSIGNMENT1/ema_model_4000_extra.pth')\n",
        "  if epoch % 10==0:\n",
        "\n",
        "    with open('/content/drive/My Drive/AML_ASSIGNMENT1/Train_Loss_4000_extra.txt','wb' )as fp:\n",
        "        pickle.dump(Train_Loss,fp)\n",
        "    with open('/content/drive/My Drive/AML_ASSIGNMENT1/Train_Loss_X_4000_extra.txt','wb') as fp:\n",
        "        pickle.dump(Train_Loss_X,fp)\n",
        "    with open('/content/drive/My Drive/AML_ASSIGNMENT1/Train_Loss_U_4000_extra.txt','wb') as fp:\n",
        "        pickle.dump(Train_Loss_U,fp)\n",
        "    with open('/content/drive/My Drive/AML_ASSIGNMENT1/Train_Accuracy_4000_extra.txt','wb' )as fp:\n",
        "        pickle.dump(Train_Accuracy,fp)\n",
        "    with open('/content/drive/My Drive/AML_ASSIGNMENT1/Test_Loss_4000_extra.txt','wb') as fp:\n",
        "        pickle.dump(Test_Loss,fp)\n",
        "    with open('/content/drive/My Drive/AML_ASSIGNMENT1/Validation_Loss_4000_extra.txt','wb') as fp:\n",
        "        pickle.dump(Validation_Loss,fp)\n",
        "    with open('/content/drive/My Drive/AML_ASSIGNMENT1/Test_Accuracy_4000_extra.txt','wb') as fp:\n",
        "        pickle.dump(Test_Accuracy,fp)\n",
        "    with open('/content/drive/My Drive/AML_ASSIGNMENT1/Validation_Accuracy_4000_extra.txt','wb') as fp:\n",
        "        pickle.dump(Validation_Accuracy,fp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch :  0\n",
            "Train_loss = 1.4586808276986043\n",
            "Valid_loss = 1.7563951077306332\n",
            "Test_loss = 1.7794467657712902\n",
            "Train_accyracy = 64.925\n",
            "Val_accuracy = 60.783510778303715\n",
            "Test_accuracy = 60.222034416831775\n",
            "Epoch :  1\n",
            "Train_loss = 1.074326799126474\n",
            "Valid_loss = 0.6128626386226932\n",
            "Test_loss = 0.6507104116857748\n",
            "Train_accyracy = 98.15\n",
            "Val_accuracy = 90.67704067704068\n",
            "Test_accuracy = 90.33497233704401\n",
            "Epoch :  2\n",
            "Train_loss = 0.9626662442093994\n",
            "Valid_loss = 0.4139634524900471\n",
            "Test_loss = 0.4470965940855585\n",
            "Train_accyracy = 99.85\n",
            "Val_accuracy = 91.8236418132277\n",
            "Test_accuracy = 91.18392746449982\n",
            "Epoch :  3\n",
            "Train_loss = 0.9411122881492057\n",
            "Valid_loss = 0.37752586639776864\n",
            "Test_loss = 0.4080889942496557\n",
            "Train_accyracy = 99.925\n",
            "Val_accuracy = 91.91919189836365\n",
            "Test_accuracy = 91.11478179762828\n",
            "Epoch :  4\n",
            "Train_loss = 0.9110702503413901\n",
            "Valid_loss = 0.3717679278368251\n",
            "Test_loss = 0.39954062177128197\n",
            "Train_accyracy = 99.975\n",
            "Val_accuracy = 91.86459184376359\n",
            "Test_accuracy = 91.08020896888175\n",
            "Epoch :  5\n",
            "Train_loss = 0.8963773865986058\n",
            "Valid_loss = 0.375586448920219\n",
            "Test_loss = 0.4008057986253892\n",
            "Train_accyracy = 100.0\n",
            "Val_accuracy = 91.45509144467732\n",
            "Test_accuracy = 90.93807620451211\n",
            "Epoch :  6\n",
            "Train_loss = 0.8934859776590752\n",
            "Valid_loss = 0.3845476935526858\n",
            "Test_loss = 0.41016316379076434\n",
            "Train_accyracy = 100.0\n",
            "Val_accuracy = 91.52334150251326\n",
            "Test_accuracy = 90.86124769514444\n",
            "Epoch :  7\n",
            "Train_loss = 0.86993581966651\n",
            "Valid_loss = 0.38021570341592925\n",
            "Test_loss = 0.40299075563480025\n",
            "Train_accyracy = 100.0\n",
            "Val_accuracy = 92.001092001092\n",
            "Test_accuracy = 91.27996311293252\n",
            "Epoch :  8\n",
            "Train_loss = 0.8570423221662934\n",
            "Valid_loss = 0.3657499260177321\n",
            "Test_loss = 0.3872271508216565\n",
            "Train_accyracy = 100.0\n",
            "Val_accuracy = 92.1785421577139\n",
            "Test_accuracy = 91.44130300698869\n",
            "Epoch :  9\n",
            "Train_loss = 0.8550008121606578\n",
            "Valid_loss = 0.3590043238479844\n",
            "Test_loss = 0.38138478466829934\n",
            "Train_accyracy = 100.0\n",
            "Val_accuracy = 92.00109199067786\n",
            "Test_accuracy = 91.35679163636792\n",
            "Epoch :  10\n",
            "Train_loss = 0.8655920301960033\n",
            "Valid_loss = 0.3558630030108969\n",
            "Test_loss = 0.3747447648331221\n",
            "Train_accyracy = 100.0\n",
            "Val_accuracy = 92.08299208299208\n",
            "Test_accuracy = 91.58727719729563\n",
            "Epoch :  11\n",
            "Train_loss = 0.8044525112142772\n",
            "Valid_loss = 0.3404070825450511\n",
            "Test_loss = 0.36032562239007415\n",
            "Train_accyracy = 100.0\n",
            "Val_accuracy = 92.39694238652827\n",
            "Test_accuracy = 91.74477565134679\n",
            "Epoch :  12\n",
            "Train_loss = 0.8266643336790753\n",
            "Valid_loss = 0.34470599170529126\n",
            "Test_loss = 0.36369623019421254\n",
            "Train_accyracy = 100.0\n",
            "Val_accuracy = 92.30139228056403\n",
            "Test_accuracy = 91.79855561915834\n",
            "Epoch :  13\n",
            "Train_loss = 0.8176587775320284\n",
            "Valid_loss = 0.3420110930763175\n",
            "Test_loss = 0.35868091081723014\n",
            "Train_accyracy = 100.0\n",
            "Val_accuracy = 92.4378924274783\n",
            "Test_accuracy = 91.98678549477566\n",
            "Epoch :  14\n",
            "Train_loss = 0.8392727901631101\n",
            "Valid_loss = 0.33991105660059856\n",
            "Test_loss = 0.35578588913317905\n",
            "Train_accyracy = 100.0\n",
            "Val_accuracy = 92.30139229097817\n",
            "Test_accuracy = 92.06361401352181\n",
            "Epoch :  15\n",
            "Train_loss = 0.8083905801067026\n",
            "Valid_loss = 0.34240566810362655\n",
            "Test_loss = 0.3581670030869072\n",
            "Train_accyracy = 100.0\n",
            "Val_accuracy = 92.50614249572837\n",
            "Test_accuracy = 91.97910264290104\n",
            "Epoch :  16\n",
            "Train_loss = 0.8350656441574198\n",
            "Valid_loss = 0.340360064650376\n",
            "Test_loss = 0.35600882319856\n",
            "Train_accyracy = 100.0\n",
            "Val_accuracy = 92.7791427583145\n",
            "Test_accuracy = 92.04824830977259\n",
            "Epoch :  17\n",
            "Train_loss = 0.8202869077139776\n",
            "Valid_loss = 0.3403618518378023\n",
            "Test_loss = 0.35656016213291614\n",
            "Train_accyracy = 100.0\n",
            "Val_accuracy = 92.94294292211468\n",
            "Test_accuracy = 92.21342962507683\n",
            "Epoch :  18\n",
            "Train_loss = 0.8383663070248807\n",
            "Valid_loss = 0.34726495177587\n",
            "Test_loss = 0.3627163502396289\n",
            "Train_accyracy = 100.0\n",
            "Val_accuracy = 92.77914277914277\n",
            "Test_accuracy = 92.17885679164105\n",
            "Epoch :  19\n",
            "Train_loss = 0.7837360739583281\n",
            "Valid_loss = 0.34325896050967125\n",
            "Test_loss = 0.3589682667696483\n",
            "Train_accyracy = 100.0\n",
            "Val_accuracy = 92.7654927446645\n",
            "Test_accuracy = 92.2211124722622\n",
            "Epoch :  20\n",
            "Train_loss = 0.8181077204021354\n",
            "Valid_loss = 0.33895900691756453\n",
            "Test_loss = 0.35218553634311206\n",
            "Train_accyracy = 100.0\n",
            "Val_accuracy = 92.99754297671473\n",
            "Test_accuracy = 92.34019667631875\n",
            "Epoch :  21\n",
            "Train_loss = 0.806551322538963\n",
            "Valid_loss = 0.33807003324779694\n",
            "Test_loss = 0.351732313523061\n",
            "Train_accyracy = 100.0\n",
            "Val_accuracy = 92.95659294617882\n",
            "Test_accuracy = 92.39781807006761\n",
            "Epoch :  22\n",
            "Train_loss = 0.8254391916323541\n",
            "Valid_loss = 0.34016813768199516\n",
            "Test_loss = 0.35249894670569687\n",
            "Train_accyracy = 100.0\n",
            "Val_accuracy = 93.12039310997899\n",
            "Test_accuracy = 92.66287645974185\n",
            "Epoch :  23\n",
            "Train_loss = 0.8269584383778861\n",
            "Valid_loss = 0.3354081130161202\n",
            "Test_loss = 0.35169632885818036\n",
            "Train_accyracy = 100.0\n",
            "Val_accuracy = 93.33879331796507\n",
            "Test_accuracy = 92.60141364005568\n",
            "Epoch :  24\n",
            "Train_loss = 0.7881910823520244\n",
            "Valid_loss = 0.34043507897525155\n",
            "Test_loss = 0.3543860347049692\n",
            "Train_accyracy = 100.0\n",
            "Val_accuracy = 92.99754297671473\n",
            "Test_accuracy = 92.62062077443147\n",
            "Epoch :  25\n",
            "Train_loss = 0.8125773532440754\n",
            "Valid_loss = 0.341353565055817\n",
            "Test_loss = 0.3567066603851377\n",
            "Train_accyracy = 100.0\n",
            "Val_accuracy = 93.06579304496479\n",
            "Test_accuracy = 92.55147510287068\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihZafP-0zrqZ"
      },
      "source": [
        "with open('/content/drive/My Drive/AML_ASSIGNMENT1/Train_Loss_250.txt','wb' )as fp:\n",
        "    pickle.dump(Train_Loss,fp)\n",
        "# with open('/content/drive/My Drive/AML_ASSIGNMENT1/Train_Loss_X_250.txt','wb') as fp:\n",
        "#     pickle.dump(Train_Loss_X,fp)\n",
        "# with open('/content/drive/My Drive/AML_ASSIGNMENT1/Train_Loss_U_250.txt','wb') as fp:\n",
        "#     pickle.dump(Train_Loss_U,fp)\n",
        "# with open('/content/drive/My Drive/AML_ASSIGNMENT1/Train_Accuracy_250.txt','wb' )as fp:\n",
        "#     pickle.dump(Train_Accuracy,fp)\n",
        "# with open('/content/drive/My Drive/AML_ASSIGNMENT1/Test_Loss_250.txt','wb') as fp:\n",
        "#     pickle.dump(Test_Loss,fp)\n",
        "# with open('/content/drive/My Drive/AML_ASSIGNMENT1/Validation_Loss_250.txt','wb') as fp:\n",
        "#     pickle.dump(Validation_Loss,fp)\n",
        "# with open('/content/drive/My Drive/AML_ASSIGNMENT1/Test_Accuracy_250.txt','wb') as fp:\n",
        "#     pickle.dump(Test_Accuracy,fp)\n",
        "# with open('/content/drive/My Drive/AML_ASSIGNMENT1/Validation_Accuracy_250.txt','wb') as fp:\n",
        "#     pickle.dump(Validation_Accuracy,fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqng4v5GRTFb"
      },
      "source": [
        "def train(labeled_trainloader, unlabeled_trainloader, model, optimizer, ema_optimizer, criterion, epoch):\n",
        "\n",
        "    losses = AverageMeter()\n",
        "    losses_x = AverageMeter()\n",
        "    losses_u = AverageMeter()\n",
        "    ws = AverageMeter()\n",
        "    labeled_train_iter = iter(labeled_trainloader)\n",
        "    unlabeled_train_iter = iter(unlabeled_trainloader)\n",
        "    model.train()\n",
        "    \n",
        "    for batch_idx in range(1024):\n",
        "        try:\n",
        "            inputs_x, targets_x = labeled_train_iter.next()\n",
        "        except:\n",
        "            labeled_train_iter = iter(labeled_trainloader)\n",
        "            inputs_x, targets_x = labeled_train_iter.next()\n",
        "\n",
        "        try:\n",
        "            (inputs_u, inputs_u2)= unlabeled_train_iter.next()\n",
        "        except:\n",
        "            unlabeled_train_iter = iter(unlabeled_trainloader)\n",
        "            (inputs_u, inputs_u2) = unlabeled_train_iter.next()\n",
        "\n",
        "\n",
        "        batch_size = inputs_x.size(0)\n",
        "        targets_x = torch.zeros(batch_size, 10).scatter_(1, targets_x.view(-1,1).long(), 1)\n",
        "\n",
        "        inputs_x, targets_x = inputs_x.cuda(), targets_x.cuda(non_blocking=True)\n",
        "        inputs_u = inputs_u.cuda()\n",
        "        inputs_u2 = inputs_u2.cuda()\n",
        "\n",
        "        inputs_u=inputs_u.float()\n",
        "        inputs_u2=inputs_u2.float()\n",
        "        inputs_x=inputs_x.float()\n",
        "        with torch.no_grad():\n",
        "            outputs_u = model(inputs_u)\n",
        "            outputs_u2 = model(inputs_u2)\n",
        "            p = (torch.softmax(outputs_u, dim=1) + torch.softmax(outputs_u2, dim=1)) / 2\n",
        "            pt = p**(1/0.5)\n",
        "            targets_u = pt / pt.sum(dim=1, keepdim=True)\n",
        "            targets_u = targets_u.detach()\n",
        "\n",
        "        # mixup\n",
        "        all_inputs = torch.cat([inputs_x, inputs_u, inputs_u2], dim=0)\n",
        "        all_targets = torch.cat([targets_x, targets_u, targets_u], dim=0)\n",
        "\n",
        "        l = np.random.beta(0.25, 0.25)\n",
        "\n",
        "        l = max(l, 1-l)\n",
        "\n",
        "        idx = torch.randperm(all_inputs.size(0))\n",
        "\n",
        "        input_a, input_b = all_inputs, all_inputs[idx]\n",
        "        target_a, target_b = all_targets, all_targets[idx]\n",
        "\n",
        "        mixed_input = l * input_a + (1 - l) * input_b\n",
        "        mixed_target = l * target_a + (1 - l) * target_b\n",
        "\n",
        "        # interleave labeled and unlabed samples between batches to get correct batchnorm calculation \n",
        "        mixed_input = list(torch.split(mixed_input, batch_size))\n",
        "        mixed_input = interleave(mixed_input, batch_size)\n",
        "\n",
        "        logits = [model(mixed_input[0])]\n",
        "        for input in mixed_input[1:]:\n",
        "            logits.append(model(input))\n",
        "\n",
        "        # put interleaved samples back\n",
        "        logits = interleave(logits, batch_size)\n",
        "        logits_x = logits[0]\n",
        "        logits_u = torch.cat(logits[1:], dim=0)\n",
        "\n",
        "        Lx, Lu, w = criterion(logits_x, mixed_target[:batch_size], logits_u, mixed_target[batch_size:], epoch+batch_idx/1024)\n",
        "\n",
        "        loss = Lx + w * Lu\n",
        "\n",
        "        # record loss\n",
        "        losses.update(loss.item(), inputs_x.size(0))\n",
        "        losses_x.update(Lx.item(), inputs_x.size(0))\n",
        "        losses_u.update(Lu.item(), inputs_x.size(0))\n",
        "        ws.update(w, inputs_x.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        ema_optimizer.step()\n",
        "\n",
        "    return (losses.avg, losses_x.avg, losses_u.avg,)\n",
        "\n",
        "def testing(dataloader, model, criterion, epoch):\n",
        "\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda(non_blocking=True)\n",
        "            # compute output\n",
        "            inputs=inputs.float()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))\n",
        "            losses.update(loss.item(), inputs.size(0))\n",
        "            top1.update(prec1.item(), inputs.size(0))\n",
        "            top5.update(prec5.item(), inputs.size(0))\n",
        "\n",
        "    return (losses.avg, top1.avg)\n",
        "\n",
        "\n",
        "def linear_rampup(current, rampup_length=1024):\n",
        "    if rampup_length == 0:\n",
        "        return 1.0\n",
        "    else:\n",
        "        current = np.clip(current / rampup_length, 0.0, 1.0)\n",
        "        return float(current)\n",
        "\n",
        "class SemiLoss(object):\n",
        "    def __call__(self, outputs_x, targets_x, outputs_u, targets_u, epoch):\n",
        "        probs_u = torch.softmax(outputs_u, dim=1)\n",
        "\n",
        "        Lx = -torch.mean(torch.sum(F.log_softmax(outputs_x, dim=1) * targets_x, dim=1))\n",
        "        Lu = torch.mean((probs_u - targets_u)**2)\n",
        "\n",
        "        return Lx, Lu, 250* linear_rampup(epoch)\n",
        "\n",
        "class WeightEMA(object):\n",
        "    def __init__(self, model, ema_model, alpha=0.999):\n",
        "        self.model = model\n",
        "        self.ema_model = ema_model\n",
        "        self.alpha = alpha\n",
        "        self.params = list(model.state_dict().values())\n",
        "        self.ema_params = list(ema_model.state_dict().values())\n",
        "        self.wd = 0.02 * 0.002\n",
        "\n",
        "        for param, ema_param in zip(self.params, self.ema_params):\n",
        "            param.data.copy_(ema_param.data)\n",
        "\n",
        "    def step(self):\n",
        "        one_minus_alpha = 1.0 - self.alpha\n",
        "        for param, ema_param in zip(self.params, self.ema_params):\n",
        "            if ema_param.dtype==torch.float32:\n",
        "                ema_param.mul_(self.alpha)\n",
        "                ema_param.add_(param * one_minus_alpha)\n",
        "                # customized weight decay\n",
        "                param.mul_(1 - self.wd)\n",
        "\n",
        "def interleave_offsets(batch, nu):\n",
        "    groups = [batch // (nu + 1)] * (nu + 1)\n",
        "    for x in range(batch - sum(groups)):\n",
        "        groups[-x - 1] += 1\n",
        "    offsets = [0]\n",
        "    for g in groups:\n",
        "        offsets.append(offsets[-1] + g)\n",
        "    assert offsets[-1] == batch\n",
        "    return offsets\n",
        "\n",
        "\n",
        "def interleave(xy, batch):\n",
        "    nu = len(xy) - 1\n",
        "    offsets = interleave_offsets(batch, nu)\n",
        "    xy = [[v[offsets[p]:offsets[p + 1]] for p in range(nu + 1)] for v in xy]\n",
        "    for i in range(1, nu + 1):\n",
        "        xy[0][i], xy[i][i] = xy[i][i], xy[0][i]\n",
        "    return [torch.cat(v, dim=0) for v in xy]\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu7Is0x7RTQI"
      },
      "source": [
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0, activate_before_residual=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes, momentum=0.001)\n",
        "        self.relu1 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_planes, momentum=0.001)\n",
        "        self.relu2 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.droprate = dropRate\n",
        "        self.equalInOut = (in_planes == out_planes)\n",
        "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
        "                               padding=0, bias=False) or None\n",
        "        self.activate_before_residual = activate_before_residual\n",
        "    def forward(self, x):\n",
        "        if not self.equalInOut and self.activate_before_residual == True:\n",
        "            x = self.relu1(self.bn1(x))\n",
        "        else:\n",
        "            out = self.relu1(self.bn1(x))\n",
        "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
        "        if self.droprate > 0:\n",
        "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
        "        out = self.conv2(out)\n",
        "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
        "\n",
        "class NetworkBlock(nn.Module):\n",
        "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0, activate_before_residual=False):\n",
        "        super(NetworkBlock, self).__init__()\n",
        "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate, activate_before_residual)\n",
        "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate, activate_before_residual):\n",
        "        layers = []\n",
        "        for i in range(int(nb_layers)):\n",
        "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate, activate_before_residual))\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, num_classes, depth=28, widen_factor=2, dropRate=0.0):\n",
        "        super(WideResNet, self).__init__()\n",
        "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
        "        assert((depth - 4) % 6 == 0)\n",
        "        n = (depth - 4) / 6\n",
        "        block = BasicBlock\n",
        "        # 1st conv before any network block\n",
        "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        # 1st block\n",
        "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate, activate_before_residual=True)\n",
        "        # 2nd block\n",
        "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
        "        # 3rd block\n",
        "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
        "        # global average pooling and classifier\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels[3], momentum=0.001)\n",
        "        self.relu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
        "        self.nChannels = nChannels[3]\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight.data)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.relu(self.bn1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(-1, self.nChannels)\n",
        "        return self.fc(out)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bKQ9LwlRVis"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}